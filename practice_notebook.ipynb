{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# We will need the following libraries:\n",
    "# pandas, numpy, matplotlib.pyplot, seaborn\n",
    "# 'import' them using the usual Python syntax\n",
    "import ??? ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and cleaning the data\n",
    "\n",
    "#### If you don't want to bother with cleaning the data (fixing missing values etc.), skip directly to the \"Implementing logistic regression\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file 'titanic_train.csv' into a Pandas dataframe.\n",
    "# ask the web, if you don't know how\n",
    "\n",
    "tdf = pd.read_csv(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas, plot a histogram of the Fare column of our dataframe 'tdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Seaborn, create a boxplot showing the age distribution\n",
    "# of passengers, by class. Look up the syntax of the function on the web.\n",
    "\n",
    "sns.boxplot(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        else:\n",
    "            return 24\n",
    "    else:\n",
    "        return Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we apply the above function, in order to substitute the missing\n",
    "# values of age with their averages based on the class\n",
    "tdf['Age'] = tdf[['Age','Pclass']].apply(impute_age,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we show if there are any more missing values\n",
    "sns.heatmap(tdf.isnull(),yticklabels=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are a lot of null values in the \"cabin\" column.\n",
    "\n",
    "Let's remove it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, drop the \"Cabin\" column from our dataframe\n",
    "# look up the syntax online\n",
    "\n",
    "tdf = tdf.drop(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this displays the dataset, without the \"Cabin\" column\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the columns and data types\n",
    "tdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for Sex and Embark, we want to turn the text variables\n",
    "# (which look like 'male'/'female' and 'S'/'C')\n",
    "# to binary variables (0/1)\n",
    "# the function pd.get_dummies is used for this\n",
    "\n",
    "sex = pd.get_dummies(tdf['Sex'], drop_first=True)\n",
    "embark = pd.get_dummies(tdf['Embarked'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the columns we don't need anymore:\n",
    "# 'Sex','Embarked','Name','Ticket'\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here, we concatenate our tdf with the two \"dummy\" columns\n",
    "\n",
    "tdf = pd.concat([tdf, sex, embark], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here, we show what's left\n",
    "\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save this\n",
    "\n",
    "tdf.to_csv(\"titanic_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression using Scikit Learn \n",
    "### Preparing the train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the cleaned data in \"titanic_cleaned.csv\"\n",
    "\n",
    "tdf = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tdf.drop('Survived',axis=1), \n",
    "                                                    tdf['Survived'], test_size=0.30, \n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "\n",
    "# fit the model, using X_train and y_train as training data\n",
    "logmodel.fit(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the results on X_test\n",
    "pred = logmodel.predict(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# run the classification report, comparing\n",
    "# y_test (the original true labels for the test set)\n",
    "# against\n",
    "# pred (the labels predicted by our model)\n",
    "\n",
    "classification_report(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# run the confusion matrix analysis,\n",
    "# agains comparing y_test and pred\n",
    "confusion_matrix(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now try the following metrics: accuracy, precision, recall.\n",
    "\n",
    "They are all available in `sklearn.metrics`. Import them and try them out."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
